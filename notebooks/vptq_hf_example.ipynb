{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/huggingface/transformers.git -U\n",
    "! pip install vptq -U\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load VPTQ-quantized model directly from HuggingFace Hub\n",
    "model = AutoModelForCausalLM.from_pretrained(\"VPTQ-community/Meta-Llama-3.1-8B-Instruct-v8-k65536-256-woft\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"VPTQ-community/Meta-Llama-3.1-8B-Instruct-v8-k65536-256-woft\")\n",
    "\n",
    "# Simple inference\n",
    "prompt = \"Explain: Do not go gentle into that good night.\"\n",
    "output = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(model.device))\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
